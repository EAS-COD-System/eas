} backup.js : #!/usr/bin/env node

const fs = require('fs-extra');
const path = require('path');
const { exec } = require('child_process');
const { v4: uuidv4 } = require('uuid');

const ROOT = __dirname;
const DATA_FILE = path.join(ROOT, 'db.json');
const BACKUP_DIR = path.join(ROOT, 'data', 'backups');
const SNAPSHOT_DIR = path.join(ROOT, 'data', 'snapshots');

// Backup destinations (configure these for your setup)
const BACKUP_DESTINATIONS = [
  // Local backup (keep 7 days)
  path.join(ROOT, 'data', 'backups', 'local'),
  
  // Add your remote backup destinations here:
  // '/path/to/network/share/eas-backups',
  // '/mnt/external-drive/eas-backups',
  // 'user@remote-server:/backup/eas-tracker'
];

// Cloud storage configurations (uncomment and configure as needed)
const CLOUD_CONFIG = {
  // AWS S3 example:
  // s3: {
  //   bucket: 'your-backup-bucket',
  //   region: 'us-east-1',
  //   accessKeyId: process.env.AWS_ACCESS_KEY_ID,
  //   secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY
  // },
  
  // Google Drive example (would need rclone setup):
  // gdrive: {
  //   remoteName: 'your-google-drive',
  //   folderId: 'your-folder-id'
  // },
  
  // Dropbox example:
  // dropbox: {
  //   accessToken: process.env.DROPBOX_ACCESS_TOKEN
  // }
};

async function ensureBackupDirs() {
  for (const dest of BACKUP_DESTINATIONS) {
    await fs.ensureDir(dest);
  }
  await fs.ensureDir(BACKUP_DIR);
}

async function createBackup() {
  try {
    console.log('üîÑ Starting EAS Tracker backup...');
    await ensureBackupDirs();
    
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const backupId = `auto-${timestamp}`;
    
    // Create backup entry in database
    const db = await fs.readJson(DATA_FILE);
    db.snapshots = db.snapshots || [];
    
    const backupEntry = {
      id: uuidv4(),
      name: backupId,
      file: path.join(BACKUP_DIR, `${backupId}.json`),
      createdAt: new Date().toISOString(),
      kind: 'auto'
    };
    
    // Create backup file
    const backupData = {
      metadata: {
        version: '1.0',
        backupId: backupId,
        timestamp: new Date().toISOString(),
        dataSize: JSON.stringify(db).length
      },
      data: db
    };
    
    const backupFile = path.join(BACKUP_DIR, `${backupId}.json`);
    await fs.writeJson(backupFile, backupData, { spaces: 2 });
    
    // Add to snapshots list
    db.snapshots.unshift(backupEntry);
    await fs.writeJson(DATA_FILE, db, { spaces: 2 });
    
    console.log('‚úÖ Local backup created:', backupId);
    
    // Copy to all backup destinations
    await copyToBackupDestinations(backupFile, backupId);
    
    // Upload to cloud storage (if configured)
    await uploadToCloud(backupFile, backupId);
    
    // Clean up old backups (keep last 30 days locally, 7 days in other destinations)
    await cleanupOldBackups();
    
    console.log('üéâ Backup completed successfully!');
    return backupId;
    
  } catch (error) {
    console.error('‚ùå Backup failed:', error.message);
    process.exit(1);
  }
}

async function copyToBackupDestinations(backupFile, backupId) {
  for (const dest of BACKUP_DESTINATIONS.slice(1)) { // Skip first (local)
    try {
      if (dest.includes('@')) {
        // Remote server via SCP
        await execPromise(`scp "${backupFile}" "${dest}/${backupId}.json"`);
      } else {
        // Local/network path
        const destFile = path.join(dest, `${backupId}.json`);
        await fs.copy(backupFile, destFile);
      }
      console.log(`‚úÖ Copied to: ${dest}`);
    } catch (error) {
      console.error(`‚ùå Failed to copy to ${dest}:`, error.message);
    }
  }
}

async function uploadToCloud(backupFile, backupId) {
  // AWS S3 Upload
  if (CLOUD_CONFIG.s3) {
    try {
      const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
      const s3Client = new S3Client(CLOUD_CONFIG.s3);
      
      const fileContent = await fs.readFile(backupFile);
      const command = new PutObjectCommand({
        Bucket: CLOUD_CONFIG.s3.bucket,
        Key: `eas-tracker/${backupId}.json`,
        Body: fileContent,
        ContentType: 'application/json'
      });
      
      await s3Client.send(command);
      console.log('‚úÖ Uploaded to AWS S3');
    } catch (error) {
      console.error('‚ùå AWS S3 upload failed:', error.message);
    }
  }
  
  // Add other cloud providers as needed
  // Google Drive, Dropbox, etc.
}

async function cleanupOldBackups() {
  const now = new Date();
  const localKeepDays = 30;
  const remoteKeepDays = 7;
  
  try {
    // Clean local backups (keep 30 days)
    const localBackupDir = BACKUP_DESTINATIONS[0];
    const files = await fs.readdir(localBackupDir);
    
    for (const file of files) {
      if (file.endsWith('.json')) {
        const filePath = path.join(localBackupDir, file);
        const stats = await fs.stat(filePath);
        const fileAgeDays = (now - stats.mtime) / (1000 * 60 * 60 * 24);
        
        if (fileAgeDays > localKeepDays) {
          await fs.remove(filePath);
          console.log(`üóëÔ∏è  Deleted old local backup: ${file}`);
        }
      }
    }
    
    // Clean main backup directory (keep 30 days)
    const backupFiles = await fs.readdir(BACKUP_DIR);
    for (const file of backupFiles) {
      if (file.endsWith('.json') && file.startsWith('auto-')) {
        const filePath = path.join(BACKUP_DIR, file);
        const stats = await fs.stat(filePath);
        const fileAgeDays = (now - stats.mtime) / (1000 * 60 * 60 * 24);
        
        if (fileAgeDays > localKeepDays) {
          await fs.remove(filePath);
          console.log(`üóëÔ∏è  Deleted old backup: ${file}`);
        }
      }
    }
    
    // Clean remote destinations (keep 7 days)
    for (const dest of BACKUP_DESTINATIONS.slice(1)) {
      if (!dest.includes('@')) {
        // Local/network path
        try {
          const files = await fs.readdir(dest);
          for (const file of files) {
            if (file.endsWith('.json')) {
              const filePath = path.join(dest, file);
              const stats = await fs.stat(filePath);
              const fileAgeDays = (now - stats.mtime) / (1000 * 60 * 60 * 24);
              
              if (fileAgeDays > remoteKeepDays) {
                await fs.remove(filePath);
                console.log(`üóëÔ∏è  Deleted old remote backup: ${file}`);
              }
            }
          }
        } catch (error) {
          console.error(`‚ùå Error cleaning ${dest}:`, error.message);
        }
      }
    }
    
  } catch (error) {
    console.error('‚ùå Cleanup error:', error.message);
  }
}

function execPromise(command) {
  return new Promise((resolve, reject) => {
    exec(command, (error, stdout, stderr) => {
      if (error) {
        reject(error);
      } else {
        resolve(stdout);
      }
    });
  });
}

// List available backups
async function listBackups() {
  try {
    console.log('üìÇ Available Backups:');
    console.log('='.repeat(50));
    
    const files = await fs.readdir(BACKUP_DIR);
    const backupFiles = files.filter(f => f.endsWith('.json')).sort().reverse();
    
    for (const file of backupFiles) {
      const filePath = path.join(BACKUP_DIR, file);
      const stats = await fs.stat(filePath);
      console.log(`üìÅ ${file}`);
      console.log(`   Size: ${(stats.size / 1024).toFixed(2)} KB`);
      console.log(`   Modified: ${stats.mtime.toLocaleString()}`);
      console.log('');
    }
    
    if (backupFiles.length === 0) {
      console.log('No backups found.');
    }
    
  } catch (error) {
    console.error('‚ùå Error listing backups:', error.message);
  }
}

// Main function
async function main() {
  const command = process.argv[2];
  
  switch (command) {
    case 'create':
    case 'c':
      await createBackup();
      break;
      
    case 'list':
    case 'ls':
      await listBackups();
      break;
      
    case 'cleanup':
      await cleanupOldBackups();
      break;
      
    case 'help':
    case 'h':
    case undefined:
      console.log(`
üõ°Ô∏è EAS Tracker Backup Manager

Usage:
  node backup.js <command>

Commands:
  create, c     Create a new backup
  list, ls      List all available backups
  cleanup       Clean up old backups manually
  help, h       Show this help message

Automated Backups:
  Add to crontab for daily backups at 2 AM:
  0 2 * * * cd /path/to/eas-tracker && node backup.js create

Backup Destinations:
  Configure BACKUP_DESTINATIONS in backup.js for:
  - Local backups (30 days retention)
  - Network shares
  - Remote servers (via SCP)
  - Cloud storage (AWS S3, etc.)

Environment Variables:
  AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY for S3
  DROPBOX_ACCESS_TOKEN for Dropbox
      `);
      break;
      
    default:
      console.error('‚ùå Unknown command:', command);
      console.log('Use "node backup.js help" for usage information.');
  }
}

if (require.main === module) {
  main().catch(console.error);
}

module.exports = { createBackup, listBackups, cleanupOldBackups }; db.json {
  "password": "eastafricashop",
  "countries": [
    "china",
    "kenya",
    "tanzania",
    "uganda",
    "zambia",
    "zimbabwe"
  ],
  "products": [],
  "productNotes": [],
  "productSellingPrices": [],
  "productOrders": [],
  "brainstorming": [],
  "testedProducts": [],
  "adspend": [],
  "deliveries": [],
  "shipments": [],
  "remittances": [],
  "refunds": [],
  "finance": {
    "categories": {
      "debit": [],
      "credit": []
    },
    "entries": []
  },
  "influencers": [],
  "influencerSpends": [],
  "snapshots": []
} index.h package.json {
  "name": "eas-tracker",
  "version": "1.0.0",
  "description": "East Africa Shop Business Tracker",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "node server.js",
    "snapshot": "node snapshot.js",
    "snapshot:create": "node snapshot.js create",
    "snapshot:list": "node snapshot.js list",
    "backup": "node backup.js",
    "backup:create": "node backup.js create",
    "backup:list": "node backup.js list",
    "backup:cleanup": "node backup.js cleanup",
    "restore": "node restore.js"
  },
  "dependencies": {
    "express": "^4.19.2",
    "fs-extra": "^11.2.0",
    "body-parser": "^1.20.2",
    "cookie-parser": "^1.4.6",
    "morgan": "^1.10.0",
    "uuid": "^9.0.1"
  },
  "devDependencies": {
    "@aws-sdk/client-s3": "^3.490.0"
  },
  "engines": {
    "node": ">=18.0.0"
  }
} product.hrestore.js #!/usr/bin/env node

const fs = require('fs-extra');
const path = require('path');
const readline = require('readline');

const ROOT = __dirname;
const DATA_FILE = path.join(ROOT, 'db.json');
const BACKUP_DIR = path.join(ROOT, 'data', 'backups');

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

function question(prompt) {
  return new Promise((resolve) => {
    rl.question(prompt, resolve);
  });
}

async function listBackups() {
  try {
    const files = await fs.readdir(BACKUP_DIR);
    const backupFiles = files.filter(f => f.endsWith('.json')).sort().reverse();
    
    console.log('\nüìÇ Available Backups:');
    console.log('='.repeat(50));
    
    backupFiles.forEach((file, index) => {
      console.log(`${index + 1}. ${file}`);
    });
    
    return backupFiles;
  } catch (error) {
    console.error('‚ùå Error listing backups:', error.message);
    return [];
  }
}

async function restoreBackup(backupFile) {
  try {
    const backupPath = path.join(BACKUP_DIR, backupFile);
    
    // Verify backup exists
    if (!await fs.pathExists(backupPath)) {
      throw new Error(`Backup file not found: ${backupFile}`);
    }
    
    // Read backup data
    const backupData = await fs.readJson(backupPath);
    
    // Verify backup structure
    if (!backupData.data || !backupData.metadata) {
      throw new Error('Invalid backup file format');
    }
    
    // Create backup of current data before restore
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const currentBackup = path.join(BACKUP_DIR, `pre-restore-${timestamp}.json`);
    const currentData = await fs.readJson(DATA_FILE);
    
    await fs.writeJson(currentBackup, {
      metadata: {
        version: '1.0',
        type: 'pre-restore',
        timestamp: new Date().toISOString(),
        originalFile: DATA_FILE
      },
      data: currentData
    }, { spaces: 2 });
    
    console.log('‚úÖ Current data backed up to:', path.basename(currentBackup));
    
    // Restore the backup
    await fs.writeJson(DATA_FILE, backupData.data, { spaces: 2 });
    
    console.log('‚úÖ Restore completed successfully!');
    console.log(`üìä Restored from: ${backupFile}`);
    console.log(`üìÖ Backup date: ${backupData.metadata.timestamp}`);
    console.log(`üíæ Data size: ${backupData.metadata.dataSize} bytes`);
    
  } catch (error) {
    console.error('‚ùå Restore failed:', error.message);
    process.exit(1);
  }
}

async function main() {
  try {
    console.log('üõ°Ô∏è EAS Tracker Restore Manager');
    console.log('='.repeat(40));
    
    // List available backups
    const backups = await listBackups();
    
    if (backups.length === 0) {
      console.log('\n‚ùå No backups found in:', BACKUP_DIR);
      process.exit(1);
    }
    
    // Ask user to select backup
    const choice = await question('\nüî¢ Enter the number of the backup to restore: ');
    const index = parseInt(choice) - 1;
    
    if (isNaN(index) || index < 0 || index >= backups.length) {
      console.log('‚ùå Invalid selection');
      process.exit(1);
    }
    
    const selectedBackup = backups[index];
    
    // Confirm restoration
    console.log(`\n‚ö†Ô∏è  WARNING: This will overwrite ALL current data!`);
    console.log(`üìÅ You are about to restore: ${selectedBackup}`);
    
    const confirm = await question('‚ùì Are you sure you want to continue? (yes/NO): ');
    
    if (confirm.toLowerCase() !== 'yes') {
      console.log('‚ùå Restore cancelled.');
      process.exit(0);
    }
    
    // Perform restore
    await restoreBackup(selectedBackup);
    
  } catch (error) {
    console.error('‚ùå Error:', error.message);
    process.exit(1);
  } finally {
    rl.close();
  }
}

if (require.main === module) {
  main().catch(console.error);
}

module.exports = { restoreBackup, listBackups }; server.js #!/usr/bin/env node

const fs = require('fs-extra');
const path = require('path');
const readline = require('readline');

const ROOT = __dirname;
const DATA_FILE = path.join(ROOT, 'db.json');
const BACKUP_DIR = path.join(ROOT, 'data', 'backups');

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

function question(prompt) {
  return new Promise((resolve) => {
    rl.question(prompt, resolve);
  });
}

async function listBackups() {
  try {
    const files = await fs.readdir(BACKUP_DIR);
    const backupFiles = files.filter(f => f.endsWith('.json')).sort().reverse();
    
    console.log('\nüìÇ Available Backups:');
    console.log('='.repeat(50));
    
    backupFiles.forEach((file, index) => {
      console.log(`${index + 1}. ${file}`);
    });
    
    return backupFiles;
  } catch (error) {
    console.error('‚ùå Error listing backups:', error.message);
    return [];
  }
}

async function restoreBackup(backupFile) {
  try {
    const backupPath = path.join(BACKUP_DIR, backupFile);
    
    // Verify backup exists
    if (!await fs.pathExists(backupPath)) {
      throw new Error(`Backup file not found: ${backupFile}`);
    }
    
    // Read backup data
    const backupData = await fs.readJson(backupPath);
    
    // Verify backup structure
    if (!backupData.data || !backupData.metadata) {
      throw new Error('Invalid backup file format');
    }
    
    // Create backup of current data before restore
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const currentBackup = path.join(BACKUP_DIR, `pre-restore-${timestamp}.json`);
    const currentData = await fs.readJson(DATA_FILE);
    
    await fs.writeJson(currentBackup, {
      metadata: {
        version: '1.0',
        type: 'pre-restore',
        timestamp: new Date().toISOString(),
        originalFile: DATA_FILE
      },
      data: currentData
    }, { spaces: 2 });
    
    console.log('‚úÖ Current data backed up to:', path.basename(currentBackup));
    
    // Restore the backup
    await fs.writeJson(DATA_FILE, backupData.data, { spaces: 2 });
    
    console.log('‚úÖ Restore completed successfully!');
    console.log(`üìä Restored from: ${backupFile}`);
    console.log(`üìÖ Backup date: ${backupData.metadata.timestamp}`);
    console.log(`üíæ Data size: ${backupData.metadata.dataSize} bytes`);
    
  } catch (error) {
    console.error('‚ùå Restore failed:', error.message);
    process.exit(1);
  }
}

async function main() {
  try {
    console.log('üõ°Ô∏è EAS Tracker Restore Manager');
    console.log('='.repeat(40));
    
    // List available backups
    const backups = await listBackups();
    
    if (backups.length === 0) {
      console.log('\n‚ùå No backups found in:', BACKUP_DIR);
      process.exit(1);
    }
    
    // Ask user to select backup
    const choice = await question('\nüî¢ Enter the number of the backup to restore: ');
    const index = parseInt(choice) - 1;
    
    if (isNaN(index) || index < 0 || index >= backups.length) {
      console.log('‚ùå Invalid selection');
      process.exit(1);
    }
    
    const selectedBackup = backups[index];
    
    // Confirm restoration
    console.log(`\n‚ö†Ô∏è  WARNING: This will overwrite ALL current data!`);
    console.log(`üìÅ You are about to restore: ${selectedBackup}`);
    
    const confirm = await question('‚ùì Are you sure you want to continue? (yes/NO): ');
    
    if (confirm.toLowerCase() !== 'yes') {
      console.log('‚ùå Restore cancelled.');
      process.exit(0);
    }
    
    // Perform restore
    await restoreBackup(selectedBackup);
    
  } catch (error) {
    console.error('‚ùå Error:', error.message);
    process.exit(1);
  } finally {
    rl.close();
  }
}

if (require.main === module) {
  main().catch(console.error);
}

module.exports = { restoreBackup, listBackups }; server.js snapshot.js const fs = require('fs-extra');
const path = require('path');
const { v4: uuidv4 } = require('uuid');

const ROOT = __dirname;
const DATA_FILE = path.join(ROOT, 'db.json');
const SNAPSHOT_DIR = path.join(ROOT, 'data', 'snapshots');

async function createSnapshot(name = null) {
  try {
    await fs.ensureDir(SNAPSHOT_DIR);
    
    const db = await fs.readJson(DATA_FILE);
    const stamp = new Date().toISOString().replace(/[:.]/g, '-');
    const snapshotName = name || `Manual-${stamp}`;
    const snapshotFile = path.join(SNAPSHOT_DIR, `${stamp}-${snapshotName.replace(/\s+/g, '_')}.json`);
    
    await fs.copy(DATA_FILE, snapshotFile);
    
    const snapshotEntry = {
      id: uuidv4(),
      name: snapshotName,
      file: snapshotFile,
      createdAt: new Date().toISOString(),
      kind: 'manual'
    };
    
    db.snapshots = db.snapshots || [];
    db.snapshots.unshift(snapshotEntry);
    await fs.writeJson(DATA_FILE, db, { spaces: 2 });
    
    console.log('‚úÖ Snapshot created successfully!');
    console.log(`üìÅ File: ${path.basename(snapshotFile)}`);
    console.log(`üìõ Name: ${snapshotName}`);
    
  } catch (error) {
    console.error('‚ùå Error creating snapshot:', error.message);
  }
}

async function listSnapshots() {
  try {
    const db = await fs.readJson(DATA_FILE);
    const snapshots = db.snapshots || [];
    
    console.log('üì∏ Available Snapshots:');
    console.log('='.repeat(50));
    
    if (snapshots.length === 0) {
      console.log('No snapshots found.');
      return;
    }
    
    snapshots.forEach((snap, index) => {
      console.log(`${index + 1}. ${snap.name}`);
      console.log(`   File: ${path.basename(snap.file)}`);
      console.log(`   Date: ${new Date(snap.createdAt).toLocaleString()}`);
      console.log(`   ID: ${snap.id}`);
      console.log('');
    });
    
  } catch (error) {
    console.error('‚ùå Error listing snapshots:', error.message);
  }
}

async function main() {
  const command = process.argv[2];
  
  switch (command) {
    case 'create':
    case 'c':
      await createSnapshot(process.argv[3]);
      break;
      
    case 'list':
    case 'ls':
      await listSnapshots();
      break;
      
    case 'help':
    case 'h':
    case undefined:
      console.log(`
üì∏ EAS Tracker Snapshot Manager

Usage:
  node snapshot.js <command> [options]

Commands:
  create [name]    Create a new snapshot
  list             List all available snapshots
  help             Show this help message

Examples:
  node snapshot.js create
  node snapshot.js create "Backup name"
  node snapshot.js list
      `);
      break;
      
    default:
      console.error('‚ùå Unknown command:', command);
      console.log('Use "node snapshot.js help" for usage information.');
  }
}

if (require.main === module) {
  main().catch(console.error);
}

module.exports = { createSnapshot, listSnapshots }; On the dashboard on to do list and weekly to do list when i add a to do list and i access the website from different phone the to do list i did it before it appears only from the phone i did it from and browser i created it from and doesn‚Äôt appear from different phone or browser 

And on products menu on all products section i want you to filter by default the products with most pieces in all countries and active and also add a filter to filter by most stock on each country so if i select Uganda for example it will appear products that have high stock at first and then go to low in stock and active also make each country columns on different  color


And on performance menu on üìà Remittance Analytics (üèÜ Filtered By Top Delivered) and üí∞ Profit by Country add new columns for profit per order and profit per piece 

And on üìà Remittance Analytics (üèÜ Filtered By Top Delivered) section i want you filter by default for products with most profit and most delivered pieces at the same time and also add option to filter by whatever I want for example i can click on the orders column and orders will appear from high to low and if i click again it will be from low to high and same thing for all columns 

Also on üìà Remittance Analytics (üèÜ Filtered By Top Delivered) section and üí∞ Profit by Country i want you to make each column on different color feel free to select the colors 


Also i can see that all of the system logic when it comes to product cost and shipping is incorrect so on these sections üìà Remittance Analytics (üèÜ Filtered By Top Delivered) on performance menu and üí∞ Profit by Country on performance menu and üìã Product Info & Analytics on products menu so these sections correct the logic on them so on product cost you will see how much we paid in total for all pieces in that product to buy it from china ( we add it when we create a stock movement ) so like that you will know product cost and for shipping you will see how much it cost to reach that country so for example if we sent 100 pieces from China to kenya for 200$ the cost will be 2$ per piece in kenya for what product and if we sent it from kenya to Tanzania 50 pieces of the same product for 150$ so that means we shipped for 3$ from kenya to tanzania so the total cost of shipping until tanzania per piece will be  2$ ( shipping from china to kenya ) + 3$( cost to ship a piece from kenya to tanzania )  = 5$ 
Total shipping for each piece in tanzania of that product is 5$ 
And also if i send from tanzania to zambia for example 25 pieces of that product for 50$ that means 2$ per piece to ship from tanzania to zambia so the total shipping per piece is 5$ + 2$ ‚Äé‚Äâ=‚Äâ$7,00 so the 5$ is total shipping until tanzania and 2$ is to ship from tanzania to zambia so the total is 7$ of shipping until Zambia so you will always follow those pieces since beginning until end and keep staking cost to ship it to give accurate cost of shipping

So please please use that logic on the sections i told you only  

And for the section on performance menu üí∞ Lifetime Product Costs Analysis and Lifetime (This Product) section on product page those both sections they will use different logic so on product cost the shipping you will add there just how much we paid for the shipments in that period so lets say in 18th of October i created a shipment movement from china to kenya of 400 pieces of a product and i added i paid for that product 3000$ on product cost china and for shipping from china to kenya i added 4000$ so immediately it will show on those 2 sections if i select that period of 18th of October immediately will show 3000$ on product cost and 4000$ on shipping even tho still havent sold any piece of it so those 2 will not be attached to the remittance and you will not see those delivered piece how much we paid on them No big no just use the logic i told you because i want to see how much am paying and keep track on that and for boxleo fees and influencer spend and  advertising spend it will be like before from the remittance in that period i select 

So please do what i told you and send me file structure and if a file need to be edited dont tell me add this code on it send it full to me fixed with all codes and keep in mind i have so many entires and data on my system please be aware of that so that when you do those edits i will not lose anything i have this system already deployed on render and i have the files on github and i already did the disk backup so that i will not lose anything on deployment am just letting you know so that when you do the edits you will bot messup anything and please first tell me all what you‚Äôre going to do then start sending what you‚Äôre going to send
